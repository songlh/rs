\documentclass[10pt]{article}
\usepackage{geometry}
\usepackage{amsmath}

\usepackage{url}
\addtolength{\oddsidemargin}{-.4in}
\addtolength{\evensidemargin}{-.4in}
\addtolength{\textwidth}{0.8in}

\addtolength{\topmargin}{-0.3in}
\addtolength{\textheight}{1.3in}
\usepackage[numbers, sort]{natbib}

\usepackage{paralist}
\newenvironment{ParaEnum}[0]{\begin{inparaenum}[(1)]}{\end{inparaenum}}

\title{\vspace{-.7in}\bf{Linhai Song - Research Statement\vspace{-.4in}}}
%\author{Guoliang Jin}
\date{}
\begin{document}
\maketitle\vspace{-.2in}

My research interests span the areas of systems, security, and software engineering.
The goal of my research is to help developers build more efficient, reliable, and secure software systems.

My dissertation research centers around software performance. 
Naturally, everyone wants software to run fast. 
Slow and inefficient software can easily frustrate end users and cause economic loss,
and it has already caused several highly publicized failures. 
My research philosophy is to view the software inefficiency problem from the perspective of combating \textit{performance bugs}.
Performance bugs are software implementation mistakes that can cause inefficient execution.
Performance bugs cannot be optimized away by state-of-practice compilers. 
Many of them are overlooked by in-house testing and go on to affect end users, 
causing severe performance degradation and a huge waste of energy in the field. 
Performance bugs are becoming more critical, with the increasing complexity of modern software and workload, 
the meager increases of single-core hardware performance, 
and pressing energy concerns. 
It is urgent to combat performance bugs.

To fight performance bugs, my methodology is to investigate existing approaches originally
designed for functional bugs and try to apply, adapt, and extend them for performance bugs.
My experience spans different
stages of combating performance bugs: 
real-world bug understanding, bug detection,
failure diagnosis, and automated bug fixing.
In particular, 
I conducted the first characteristics study on performance bugs, 
based on 110 bugs randomly collected from five representative open-source software suites~\cite{jin12perfbug}.
I built a static performance bug detection tool suite by using extracted efficiency rules from fixed performance bugs~\cite{jin12perfbug} 
and helped build a dynamic performance bug detection tool for inefficient nested loops~\cite{Nistor13ICSE}. 
I explored how to apply statistical debugging to performance failure diagnosis~\cite{Song14OOPSLA}, 
and designed a series of static-dynamic hybrid analysis routines to provide more detailed diagnosis information for inefficient loops~\cite{Song17ICSE}.
I designed three source-to-source code transformations to automatically fix performance bugs in parallel applications, 
which offload computation to Intel Xeon Phi manycore coprocessors~\cite{Song14MICRO}. 

In addition to my work on software efficiency, I also worked on two software reliability projects as part of my doctoral research. In the first, 
I helped build a concurrency bug fixing system~\cite{jin11afix}, 
which can automatically eliminate atomicity-violation bugs.
In the second, I studied change histories of critical sections in open-source software~\cite{Gu15FSE} 
to provide a better understanding of synchronization challenges faced by real-world developers. 

After graduation, I worked in the area of security, 
with a focus on applying big data techniques to analyze and predict security incidents. 
Using the data repository VirusTotal, 
which contains billions of real-world malware labeled by state-of-the-art anti-virus engines, 
I studied characteristics of real-world malware, 
modeled how influence propagates across different anti-virus vendors, 
and explored the feasibility of building machine learning malware detectors based only on hash values of files~\cite{Song16ApSys,Song17EuroSys}. 
The study results have shed light on directions for future research  
and are assisting both anti-virus vendors and normal users in their fight against malware.

My research work has already had industrial and academic impact. 
My two performance bug detection techniques~\cite{jin12perfbug, Nistor13ICSE} 
found hundreds of previously unknown performance bugs in mature open-source software, 
many of which have already been confirmed and fixed by developers. 
My performance bug fixing project~\cite{Song14MICRO} was runner-up in the competition for best paper at MICRO-47 (2014)
and has also led to an issued patent~\cite{comppatent}.
My concurrency bug fixing system~\cite{jin11afix} won the ACM SIGPLAN Research Highlights Award~\cite{afixnom}. 
As of now, my publications have 415 citations in total. 

\vspace{-.1in}
\section{My PhD Work: Combating Real-World Performance Bugs}

My PhD work is mainly focused on addressing the software 
inefficiency problem from the perspective of combating performance bugs. 
Performance bugs are software defects that can cause inefficient execution. 
My methodology is to investigate existing approaches originally designed for functional bugs
and try to apply, adapt, and extend them for performance bugs. 


\vspace{-.1in}
\paragraph{Real-world Performance Bug Understanding.}
Research on performance bugs, like that on functional bugs, 
should be guided by empirical studies. 
In order to improve the understanding of real-world performance bugs, 
I co-led what we believe to be the first 
empirical study of real-world performance bugs, 
based on 110 bugs randomly sampled from five open-source software suites~\cite{jin12perfbug}. 
Our study examined four dimensions 
that cover the complete lifetime of performance bugs:
their root causes, 
how they are introduced, their triggering conditions, and their fixing strategies. 
Our study provides many interesting and important findings, 
such as many performance bugs are introduced by workload mismatch 
and misunderstanding of APIs' performance features; 
and around half of the studied performance bugs 
will only manifest under inputs with both special features and large scales. 
Our study can guide future research on performance bugs.
According to Google Scholar, our study has already been cited 
242 times since 2012. 
It has already motivated our own performance bug detection 
and performance failure diagnosis projects.



\vspace{-.1in}
\paragraph{Static/Dynamic Performance Bug Detection.}
Bug detection aims to find previous unknown bugs,
before they manifest in front of end users. 
Rule-based detection techniques have been widely used to identify 
functional bugs in previous literatures.
Our study shows that both statically checkable efficiency rules 
and violations of these rules exist widely in software. 
Inspired by this finding,
we built a suite of rule-based static checkers for performance problems~\cite{jin12perfbug}.
In total, our checkers found 332 previously unknown performance bugs 
from the latest versions of Apache, Mozilla, and MySQL. 
We reported some of the identified bugs to developers. 
Of these, 77 have been confirmed by developers so far, 
and 15 reported bugs have already been fixed.
Our empirical study also finds that 90\% of performance bugs involve loops, 
and 50\% of performance bugs involve at least two levels of loops. 
Motivated by this finding, I helped a fellow graduate student 
build a novel automated test oracle named Toddler~\cite{Nistor13ICSE},
which can leverage the well-established testing process for functional bugs to 
identify performance bugs caused by inefficient nested loops. 
Using Toddler, we found 42 new bugs in six Java projects.
Based on our bug reports, developers so far have fixed 21 of these bugs 
and confirmed 6 more as real bugs. 

\vspace{-.1in}
\paragraph{Performance Failure Diagnosis.}
As another key aspect of fighting performance problems,
diagnosing a user-perceived performance failure is to figure out which code 
region is responsible for the observed slowdown and 
to design a patch to optimize the code region.
To improve performance failure diagnosis, 
we first explored how to 
apply statistical debugging to performance bugs~\cite{Song14OOPSLA}. 
Our empirical study shows that performance failures are usually observed through comparison,
and when reporting a performance failure, besides a bug-triggering input, 
the user usually provides another input similar to the bad one, 
but with a much better performance.
Therefore, statistical debugging is a natural fit for performance problems. 
After evaluating widely used predicates and 
two representative statistical models, 
we found that branch predicate plus the two statistical models can effectively 
pinpoint wrong branch decisions and inefficient loops, 
causing the diagnosed performance failures. 
We also found that
due to the unique nature of inefficient loops, 
sampling can lower runtime overhead without sacrificing diagnosis latency, 
which is very different from functional failure diagnosis.
We then built LDoctor~\cite{Song17ICSE} to provide more 
fine-grained diagnosis information and fixing strategies for inefficient loops. 
LDoctor takes suspicious loops reported by 
statistical debugging as inputs, 
applies hybridized static and dynamic analysis to 
balance accuracy and performance, 
and relies on sampling and other designed optimizations 
to further reduce runtime overhead. 
Evaluation results under real-world inefficient loops 
show that LDoctor covers most common root causes, 
reports few false positives, and incurs a low runtime overhead. 

\vspace{-.1in}
\paragraph{Performance Bug Fixing.}
Intel Xeon Phi coprocessors have recently been introduced 
as new members of the manycore family.
Compared with GPUs, Xeon Phi coprocessors are easier to program,
because they provide x86 compatibility and support many different programming models.
Adding simple pragmas can help developers offload existing parallel loops 
to Xeon Phi coprocessors. 
However, this does not result in better performance, 
and too many performance bugs are contained in 
offloaded parallel loops. 
After careful investigation, we designed three source-to-source code transformations to 
hide data transfer overhead between CPUs and coprocessors, 
to regularize loops with irregular memory accesses, 
and to efficiently transfer large pointer-based data structures, respectively~\cite{Song14MICRO}.
Our transformations can automatically fix performance bugs contained in offloaded parallel loops. 
Experimental results show that the designed transformations can benefit 9 
out of 12 evaluated benchmarks and improve performance by 1.16x - 52.21x. 
This work won MICRO-47 (2014) best paper runner-up.


\vspace{-.1in}
\section{Current and Near-Future Research Directions (Post-PhD)}
After joining the Pennsylvania State University as an assistant professor,
I have expanded my research directions to investigate
emerging reliability and security issues.  
In the following, I will focus on two main research themes 
to discuss my initial progress and my plans for the near future. 

\vspace{-.1in}
\subsection{Issues in New Programming Languages}
Many new programming languages have been created since 2000, 
with unique language features trying to accomplish their desired design goals.
With increasing adoption, it is important to understand 
whether the design goals are really achieved 
and whether there are unique bugs in these languages. 
Working together with my collaborator (Yiying Zhang from Purdue University), 
I mainly look into Go and Rust, because 
these two languages are popular choices 
of developers when implementing 
data center applications and safety-critical systems. 
 
\vspace{-.1in}
\paragraph{Understanding Concurrency Issues in Go}
Go is a statically-typed new programming language that aims to provide 
a simple, efficient, and safe way to build multi-threaded software.
For this purpose, Go centers its multi-threading design 
around two principles: 1) making threads (called goroutines) lightweight 
and easy to create and 2) using explicit messaging (called channel) 
to communicate across threads. With these design principles, 
Go proposes not only new primitives and new libraries 
but also new implementation of existing semantics.
Due to its powerful and expressive concurrency mechanisms, 
Go is widely used in data center environments,
like building container systems and implementing microservices.

It is important to understand how Go's new concurrency mechanisms impact concurrency bugs, 
since concurrency bugs are difficult to debug and severely degrade 
the reliability of concurrent systems. 
We conducted the first empirical study on Go concurrency bugs, based on 171 bugs collected from  
six famous Go software 
including Docker, Kubernetes and gRPC~\cite{go-asplos}.
More than half of the studied bugs are caused by non-traditional, Go-specific problems. 
Apart from root causes of these bugs, we also studied their fixes, 
performed experiments to reproduce them, 
and evaluated them with two publicly-available Go bug detectors.
Overall, our study provides a better understanding on Go's concurrency models
with many valuable findings and implications, such as message passing is more likely to 
cause blocking bugs and new programming models
that Go introduced to ease multi-threaded programming can themselves 
become the reasons of more concurrency bugs. 
Our study can guide future researchers and practitioners in writing better, 
more reliable Go software and in developing novel debugging tools for Go.

\vspace{-.1in}
\paragraph{Understanding Safety Issues in Rust}
Rust is a young programming language designed for systems software development. 
It aims to provide safety guarantees like high-level languages 
and performance efficiency
like low-level languages. The core design of Rust is a set
of strict safety rules enforced by compile-time checking. To
support more low-level controls, Rust allows programmers
to bypass these compiler checks to write unsafe code. 
Programmers can also encapsulate their unsafe code within safe
function interfaces (i.e., interior unsafe).

It is important to understand how well Rust’s safety (and
unsafe) mechanisms work in real programs. We performed
the first empirical study of Rust by close, manual inspection of 583 unsafe code usages 
and 103 bugs in five opensource Rust projects, five widely-used Rust libraries, and two
online security databases~\cite{sosp-boqin}. Our study answers three important 
questions: how and why do programmers write unsafe
code, what memory-safety issues real Rust programs have,
and what concurrency bugs Rust programmers make. 
We find that programmmers often choose to write unsafe code for good performance 
and ease of programming, although all memory-safety bugs involve unsafe code, 
most of them also involve safe code, and 
most concurrency bugs are caused by programmers' misunderstanding of Rust's safety features 
and compiler rules. 
For all the above three aspects, we make insightful suggestions 
to future Rust programmers and language developers. 
One key take-away is that a theoretically proven safe language may be hard to 
use in reality, and as a result, the language includes less safe features, which eventually cause
problems in the real world.

\section{Future Research}

Going forward, I hope to leverage my areas of expertise to improve the performance, 
reliability, and security of various types of computer systems. 

\vspace{-.1in}
\paragraph{Performance.} 
My intermediate research goal is to advance new aspects of combating performance bugs.
For example, I would like to explore how to monitor algorithmic complexity during production runs. 
On-line algorithmic monitoring can help developers understand real-world workloads, 
identify codes in superlinear complexity, 
and expose new performance optimization opportunities. 
Current methods of algorithmic profiling or monitoring incur more than 10x runtime overhead, which cannot be tolerated in production runs. 
I plan to build a lower-overhead tool that will collect runtime information 
and infer approximate complexity from deployed software. 
I would also like to provide tool support for performance testing.
Our empirical study shows that almost half of studied performance bugs only manifest 
under inputs with both special features and large scales.
Existing techniques, designed to generate inputs with good code coverage, focus only on special features.
I plan to extend these existing input generation techniques with an emphasis on large scales. 
Another important challenge that arises during performance testing is how to automatically judge whether a performance bug has occurred. 
I plan to leverage existing dynamic performance bug detection techniques to build performance testing oracles.

\vspace{-.1in}
\paragraph{Reliability.} 
As big data is changing businesses across the board, 
more and more developers are working on using big data computing systems, 
like Hadoop and Spark, to process their massive amounts of data. 
My own experience in leveraging Spark to analyze VirusTotal's data 
has given me insight into the challenges of debugging big data applications. 
First, it is time-consuming to repeat a failure, and it may take developers several hours to receive notice of a runtime failure or an incorrect output.
Second, without tool support it is almost impossible to identify failure-triggering inputs among millions of input records. 
Third, it is difficult to identify a failure's root cause, since the failure may propagate across different stages and across different nodes.

My longer term research direction is to provide tool support for debugging big data applications.
I plan to start by studying real-world bugs in big data applications to figure out whether there are common bug patterns. 
Then I hope to build static bug detectors that can identify bugs following common patterns before big data applications are executed.  
To quickly identify the root causes of failures, I plan to build slicing tools that can analyze execution across different nodes, 
interactive breakpoint tools that can stop the execution for single records specified by developers,
and log mining tools that can analyze large quantities of logs in big data applications.
Finally, I hope to build automatic fixing tools that can patch bugs in big data applications.
Since it is time-consuming to run a patched version from the very beginning after fixes have been applied, 
I hope my fixing tools can provide support to reuse the computation completed during failure runs.  


\vspace{-.1in}
\paragraph{Security.} 

I plan to continue my research on security along two directions. 
One is to apply big data techniques to security, 
and the other is to detect vulnerabilities in devices from the Internet of Things (IoT) ecosystem.

Big security data provides opportunities to leverage data-centric methods to improve today's security techniques. 
For example, we can study what attackers have done on a large scale to figure out their strategies and predict what they will do in the future. 
In addition, by utilizing malware that has already been studied and labeled by security experts, 
we can learn how malware evolves and quickly detect emerging malware. 
We can also apply deep learning techniques on these labeled malware samples and build new malware detectors. 
It is also important to think about whether existing big data computing systems are suitable for security workload
and to look for opportunities to improve current systems.

Vulnerability detection in the IoT ecosystem is more critical than ever before.
Billions of connected devices have already been in use worldwide, 
and this number is growing rapidly.  
Hacked devices can leak sensitive information and can power denial of service attacks. 
Due to the limited computation resources on a single device, 
anti-virus software cannot be used to prevent attacks. 
To make things worse, the developers who build firmware for these devices often treat security as an afterthought. 
The need to quickly detect and patch vulnerabilities is increasing dramatically. 
To detect vulnerabilities, I plan to study mechanisms for various firmware and devices 
and to extend existing techniques, like symbolic execution and whitebox fuzzing. 
Once a new vulnerability has been identified, it is also important to quickly search affected devices. 
To that end, I plan to build an efficient bug search system for IoT products.  


\newpage
\bibliographystyle{plainyr-rev}
\bibliography{rs}
\end{document}
